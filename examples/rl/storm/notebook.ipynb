{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from omegaconf import OmegaConf\n",
    "import gymnasium\n",
    "from world_models import WorldModel\n",
    "\n",
    "import colorama\n",
    "import numpy as np\n",
    "from tinygrad import Tensor, nn\n",
    "from tinygrad.nn.state import get_parameters, get_state_dict\n",
    "\n",
    "import env_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_env(env_name, image_size, seed):\n",
    "  env = gymnasium.make(env_name, full_action_space=False, render_mode=\"rgb_array\", frameskip=1)\n",
    "  env = env_wrapper.SeedEnvWrapper(env, seed=seed)\n",
    "  env = env_wrapper.MaxLast2FrameSkipWrapper(env, skip=4)\n",
    "  env = gymnasium.wrappers.ResizeObservation(env, shape=image_size)\n",
    "  env = env_wrapper.LifeLossInfo(env)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_world_model(conf, action_dim):\n",
    "    return WorldModel(\n",
    "        in_channels=conf.models.world_model.in_channels,\n",
    "        action_dim=action_dim,\n",
    "        transformer_max_length=conf.models.world_model.transformer_max_length,\n",
    "        transformer_hidden_dim=conf.models.world_model.transformer_hidden_dim,\n",
    "        transformer_num_layers=conf.models.world_model.transformer_num_layers,\n",
    "        transformer_num_heads=conf.models.world_model.transformer_num_heads\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-n\", type=str, required=True)\n",
    "parser.add_argument(\"-seed\", type=int, required=True)\n",
    "parser.add_argument(\"-config_path\", type=str, required=True)\n",
    "parser.add_argument(\"-env_name\", type=str, required=True)\n",
    "parser.add_argument(\"-trajectory_path\", type=str, required=True)\n",
    "args = parser.parse_args([\n",
    "    \"-n\", \"MsPacman\",\n",
    "    \"-seed\", \"1\",\n",
    "    \"-config_path\", \"STORM.yaml\",\n",
    "    \"-env_name\", \"ALE/MsPacman-v5\",\n",
    "    \"-trajectory_path\", \"D_TRAJ/MsPacman.pkl\"\n",
    "])\n",
    "conf = OmegaConf.load(args.config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mNamespace(n='MsPacman', seed=1, config_path='STORM.yaml', env_name='ALE/MsPacman-v5', trajectory_path='D_TRAJ/MsPacman.pkl')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colorama.Fore.RED + str(args) + colorama.Style.RESET_ALL)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "Tensor.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "dummy_env = build_single_env(args.env_name, conf.basic_settings.image_size, seed=0)\n",
    "action_dim = dummy_env.action_space.n.item()\n",
    "del dummy_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model = build_world_model(conf, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -  encoder.backbone.0.weight :  (32, 3, 4, 4)  --  None\n",
      "2 -  encoder.backbone.1.weight :  (32,)  --  None\n",
      "3 -  encoder.backbone.1.bias :  (32,)  --  None\n",
      "4 -  encoder.backbone.1.running_mean :  (32,)  --  False\n",
      "5 -  encoder.backbone.1.running_var :  (32,)  --  False\n",
      "6 -  encoder.backbone.1.num_batches_tracked :  (1,)  --  False\n",
      "7 -  encoder.backbone.3.weight :  (64, 32, 4, 4)  --  None\n",
      "8 -  encoder.backbone.4.weight :  (64,)  --  None\n",
      "9 -  encoder.backbone.4.bias :  (64,)  --  None\n",
      "10 -  encoder.backbone.4.running_mean :  (64,)  --  False\n",
      "11 -  encoder.backbone.4.running_var :  (64,)  --  False\n",
      "12 -  encoder.backbone.4.num_batches_tracked :  (1,)  --  False\n",
      "13 -  encoder.backbone.6.weight :  (128, 64, 4, 4)  --  None\n",
      "14 -  encoder.backbone.7.weight :  (128,)  --  None\n",
      "15 -  encoder.backbone.7.bias :  (128,)  --  None\n",
      "16 -  encoder.backbone.7.running_mean :  (128,)  --  False\n",
      "17 -  encoder.backbone.7.running_var :  (128,)  --  False\n",
      "18 -  encoder.backbone.7.num_batches_tracked :  (1,)  --  False\n",
      "19 -  encoder.backbone.9.weight :  (256, 128, 4, 4)  --  None\n",
      "20 -  encoder.backbone.10.weight :  (256,)  --  None\n",
      "21 -  encoder.backbone.10.bias :  (256,)  --  None\n",
      "22 -  encoder.backbone.10.running_mean :  (256,)  --  False\n",
      "23 -  encoder.backbone.10.running_var :  (256,)  --  False\n",
      "24 -  encoder.backbone.10.num_batches_tracked :  (1,)  --  False\n",
      "25 -  storm_transformer.stem.0.weight :  (512, 1033)  --  None\n",
      "26 -  storm_transformer.stem.1.weight :  (512,)  --  None\n",
      "27 -  storm_transformer.stem.1.bias :  (512,)  --  None\n",
      "28 -  storm_transformer.stem.3.weight :  (512, 512)  --  None\n",
      "29 -  storm_transformer.stem.4.weight :  (512,)  --  None\n",
      "30 -  storm_transformer.stem.4.bias :  (512,)  --  None\n",
      "31 -  storm_transformer.position_encoding.pos_emb.weight :  (64, 512)  --  None\n",
      "32 -  storm_transformer.layer_stack.0.slf_attn.w_qs.weight :  (512, 512)  --  None\n",
      "33 -  storm_transformer.layer_stack.0.slf_attn.w_ks.weight :  (512, 512)  --  None\n",
      "34 -  storm_transformer.layer_stack.0.slf_attn.w_vs.weight :  (512, 512)  --  None\n",
      "35 -  storm_transformer.layer_stack.0.slf_attn.fc.weight :  (512, 512)  --  None\n",
      "36 -  storm_transformer.layer_stack.0.slf_attn.layer_norm.weight :  (512,)  --  None\n",
      "37 -  storm_transformer.layer_stack.0.slf_attn.layer_norm.bias :  (512,)  --  None\n",
      "38 -  storm_transformer.layer_stack.0.pos_ffn.w_1.weight :  (1024, 512)  --  None\n",
      "39 -  storm_transformer.layer_stack.0.pos_ffn.w_1.bias :  (1024,)  --  None\n",
      "40 -  storm_transformer.layer_stack.0.pos_ffn.w_2.weight :  (512, 1024)  --  None\n",
      "41 -  storm_transformer.layer_stack.0.pos_ffn.w_2.bias :  (512,)  --  None\n",
      "42 -  storm_transformer.layer_stack.0.pos_ffn.layer_norm.weight :  (512,)  --  None\n",
      "43 -  storm_transformer.layer_stack.0.pos_ffn.layer_norm.bias :  (512,)  --  None\n",
      "44 -  storm_transformer.layer_stack.1.slf_attn.w_qs.weight :  (512, 512)  --  None\n",
      "45 -  storm_transformer.layer_stack.1.slf_attn.w_ks.weight :  (512, 512)  --  None\n",
      "46 -  storm_transformer.layer_stack.1.slf_attn.w_vs.weight :  (512, 512)  --  None\n",
      "47 -  storm_transformer.layer_stack.1.slf_attn.fc.weight :  (512, 512)  --  None\n",
      "48 -  storm_transformer.layer_stack.1.slf_attn.layer_norm.weight :  (512,)  --  None\n",
      "49 -  storm_transformer.layer_stack.1.slf_attn.layer_norm.bias :  (512,)  --  None\n",
      "50 -  storm_transformer.layer_stack.1.pos_ffn.w_1.weight :  (1024, 512)  --  None\n",
      "51 -  storm_transformer.layer_stack.1.pos_ffn.w_1.bias :  (1024,)  --  None\n",
      "52 -  storm_transformer.layer_stack.1.pos_ffn.w_2.weight :  (512, 1024)  --  None\n",
      "53 -  storm_transformer.layer_stack.1.pos_ffn.w_2.bias :  (512,)  --  None\n",
      "54 -  storm_transformer.layer_stack.1.pos_ffn.layer_norm.weight :  (512,)  --  None\n",
      "55 -  storm_transformer.layer_stack.1.pos_ffn.layer_norm.bias :  (512,)  --  None\n",
      "56 -  storm_transformer.layer_norm.weight :  (512,)  --  None\n",
      "57 -  storm_transformer.layer_norm.bias :  (512,)  --  None\n",
      "58 -  dist_head.post_head.weight :  (1024, 4096)  --  None\n",
      "59 -  dist_head.post_head.bias :  (1024,)  --  None\n",
      "60 -  dist_head.prior_head.weight :  (1024, 512)  --  None\n",
      "61 -  dist_head.prior_head.bias :  (1024,)  --  None\n",
      "62 -  image_decoder.backbone.0.weight :  (4096, 1024)  --  None\n",
      "63 -  image_decoder.backbone.2.weight :  (256,)  --  None\n",
      "64 -  image_decoder.backbone.2.bias :  (256,)  --  None\n",
      "65 -  image_decoder.backbone.2.running_mean :  (256,)  --  False\n",
      "66 -  image_decoder.backbone.2.running_var :  (256,)  --  False\n",
      "67 -  image_decoder.backbone.2.num_batches_tracked :  (1,)  --  False\n",
      "68 -  image_decoder.backbone.4.weight :  (256, 128, 4, 4)  --  None\n",
      "69 -  image_decoder.backbone.5.weight :  (128,)  --  None\n",
      "70 -  image_decoder.backbone.5.bias :  (128,)  --  None\n",
      "71 -  image_decoder.backbone.5.running_mean :  (128,)  --  False\n",
      "72 -  image_decoder.backbone.5.running_var :  (128,)  --  False\n",
      "73 -  image_decoder.backbone.5.num_batches_tracked :  (1,)  --  False\n",
      "74 -  image_decoder.backbone.7.weight :  (128, 64, 4, 4)  --  None\n",
      "75 -  image_decoder.backbone.8.weight :  (64,)  --  None\n",
      "76 -  image_decoder.backbone.8.bias :  (64,)  --  None\n",
      "77 -  image_decoder.backbone.8.running_mean :  (64,)  --  False\n",
      "78 -  image_decoder.backbone.8.running_var :  (64,)  --  False\n",
      "79 -  image_decoder.backbone.8.num_batches_tracked :  (1,)  --  False\n",
      "80 -  image_decoder.backbone.10.weight :  (64, 32, 4, 4)  --  None\n",
      "81 -  image_decoder.backbone.11.weight :  (32,)  --  None\n",
      "82 -  image_decoder.backbone.11.bias :  (32,)  --  None\n",
      "83 -  image_decoder.backbone.11.running_mean :  (32,)  --  False\n",
      "84 -  image_decoder.backbone.11.running_var :  (32,)  --  False\n",
      "85 -  image_decoder.backbone.11.num_batches_tracked :  (1,)  --  False\n",
      "86 -  image_decoder.backbone.13.weight :  (32, 3, 4, 4)  --  None\n",
      "87 -  image_decoder.backbone.13.bias :  (3,)  --  None\n",
      "88 -  reward_decoder.backbone.0.weight :  (512, 512)  --  None\n",
      "89 -  reward_decoder.backbone.1.weight :  (512,)  --  None\n",
      "90 -  reward_decoder.backbone.1.bias :  (512,)  --  None\n",
      "91 -  reward_decoder.backbone.3.weight :  (512, 512)  --  None\n",
      "92 -  reward_decoder.backbone.4.weight :  (512,)  --  None\n",
      "93 -  reward_decoder.backbone.4.bias :  (512,)  --  None\n",
      "94 -  reward_decoder.head.weight :  (255, 512)  --  None\n",
      "95 -  reward_decoder.head.bias :  (255,)  --  None\n",
      "96 -  termination_decoder.backbone.0.weight :  (512, 512)  --  None\n",
      "97 -  termination_decoder.backbone.1.weight :  (512,)  --  None\n",
      "98 -  termination_decoder.backbone.1.bias :  (512,)  --  None\n",
      "99 -  termination_decoder.backbone.3.weight :  (512, 512)  --  None\n",
      "100 -  termination_decoder.backbone.4.weight :  (512,)  --  None\n",
      "101 -  termination_decoder.backbone.4.bias :  (512,)  --  None\n",
      "102 -  termination_decoder.head.0.weight :  (1, 512)  --  None\n",
      "103 -  termination_decoder.head.0.bias :  (1,)  --  None\n",
      "104 -  symlog_twohot_loss_func.bins :  (255,)  --  False\n",
      "Total updatable parameters:  79\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "updatable_count = 0\n",
    "for k, v in get_state_dict(world_model).items():\n",
    "    print(i, \"- \", k, \": \", v.shape, \" -- \", v.requires_grad)\n",
    "    if v.requires_grad is None: updatable_count += 1\n",
    "    i+=1\n",
    "print(\"Total updatable parameters: \", updatable_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World model safetensors from torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {}\n",
    "with safe_open(\"world_model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        state_dict[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dist_head.post_head.bias', 'dist_head.post_head.weight', 'dist_head.prior_head.bias', 'dist_head.prior_head.weight', 'encoder.backbone.0.weight', 'encoder.backbone.1.bias', 'encoder.backbone.1.num_batches_tracked', 'encoder.backbone.1.running_mean', 'encoder.backbone.1.running_var', 'encoder.backbone.1.weight', 'encoder.backbone.10.bias', 'encoder.backbone.10.num_batches_tracked', 'encoder.backbone.10.running_mean', 'encoder.backbone.10.running_var', 'encoder.backbone.10.weight', 'encoder.backbone.3.weight', 'encoder.backbone.4.bias', 'encoder.backbone.4.num_batches_tracked', 'encoder.backbone.4.running_mean', 'encoder.backbone.4.running_var', 'encoder.backbone.4.weight', 'encoder.backbone.6.weight', 'encoder.backbone.7.bias', 'encoder.backbone.7.num_batches_tracked', 'encoder.backbone.7.running_mean', 'encoder.backbone.7.running_var', 'encoder.backbone.7.weight', 'encoder.backbone.9.weight', 'image_decoder.backbone.0.weight', 'image_decoder.backbone.10.weight', 'image_decoder.backbone.11.bias', 'image_decoder.backbone.11.num_batches_tracked', 'image_decoder.backbone.11.running_mean', 'image_decoder.backbone.11.running_var', 'image_decoder.backbone.11.weight', 'image_decoder.backbone.13.bias', 'image_decoder.backbone.13.weight', 'image_decoder.backbone.2.bias', 'image_decoder.backbone.2.num_batches_tracked', 'image_decoder.backbone.2.running_mean', 'image_decoder.backbone.2.running_var', 'image_decoder.backbone.2.weight', 'image_decoder.backbone.4.weight', 'image_decoder.backbone.5.bias', 'image_decoder.backbone.5.num_batches_tracked', 'image_decoder.backbone.5.running_mean', 'image_decoder.backbone.5.running_var', 'image_decoder.backbone.5.weight', 'image_decoder.backbone.7.weight', 'image_decoder.backbone.8.bias', 'image_decoder.backbone.8.num_batches_tracked', 'image_decoder.backbone.8.running_mean', 'image_decoder.backbone.8.running_var', 'image_decoder.backbone.8.weight', 'reward_decoder.backbone.0.weight', 'reward_decoder.backbone.1.bias', 'reward_decoder.backbone.1.weight', 'reward_decoder.backbone.3.weight', 'reward_decoder.backbone.4.bias', 'reward_decoder.backbone.4.weight', 'reward_decoder.head.bias', 'reward_decoder.head.weight', 'storm_transformer.layer_norm.bias', 'storm_transformer.layer_norm.weight', 'storm_transformer.layer_stack.0.pos_ffn.layer_norm.bias', 'storm_transformer.layer_stack.0.pos_ffn.layer_norm.weight', 'storm_transformer.layer_stack.0.pos_ffn.w_1.bias', 'storm_transformer.layer_stack.0.pos_ffn.w_1.weight', 'storm_transformer.layer_stack.0.pos_ffn.w_2.bias', 'storm_transformer.layer_stack.0.pos_ffn.w_2.weight', 'storm_transformer.layer_stack.0.slf_attn.fc.weight', 'storm_transformer.layer_stack.0.slf_attn.layer_norm.bias', 'storm_transformer.layer_stack.0.slf_attn.layer_norm.weight', 'storm_transformer.layer_stack.0.slf_attn.w_ks.weight', 'storm_transformer.layer_stack.0.slf_attn.w_qs.weight', 'storm_transformer.layer_stack.0.slf_attn.w_vs.weight', 'storm_transformer.layer_stack.1.pos_ffn.layer_norm.bias', 'storm_transformer.layer_stack.1.pos_ffn.layer_norm.weight', 'storm_transformer.layer_stack.1.pos_ffn.w_1.bias', 'storm_transformer.layer_stack.1.pos_ffn.w_1.weight', 'storm_transformer.layer_stack.1.pos_ffn.w_2.bias', 'storm_transformer.layer_stack.1.pos_ffn.w_2.weight', 'storm_transformer.layer_stack.1.slf_attn.fc.weight', 'storm_transformer.layer_stack.1.slf_attn.layer_norm.bias', 'storm_transformer.layer_stack.1.slf_attn.layer_norm.weight', 'storm_transformer.layer_stack.1.slf_attn.w_ks.weight', 'storm_transformer.layer_stack.1.slf_attn.w_qs.weight', 'storm_transformer.layer_stack.1.slf_attn.w_vs.weight', 'storm_transformer.position_encoding.pos_emb.weight', 'storm_transformer.stem.0.weight', 'storm_transformer.stem.1.bias', 'storm_transformer.stem.1.weight', 'storm_transformer.stem.3.weight', 'storm_transformer.stem.4.bias', 'storm_transformer.stem.4.weight', 'termination_decoder.backbone.0.weight', 'termination_decoder.backbone.1.bias', 'termination_decoder.backbone.1.weight', 'termination_decoder.backbone.3.weight', 'termination_decoder.backbone.4.bias', 'termination_decoder.backbone.4.weight', 'termination_decoder.head.0.bias', 'termination_decoder.head.0.weight'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CUDA'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(state_dict['dist_head.post_head.bias'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in get_state_dict(world_model).items():\n",
    "    if k in state_dict.keys():\n",
    "        print(k, \" -- \", \"yes\")\n",
    "    else:\n",
    "        print(k, \" -- \", \"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/104 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 1066.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_state_dict = get_state_dict(world_model)\n",
    "for k, v in tqdm(model_state_dict.items()):\n",
    "    if k in state_dict.keys():\n",
    "        if state_dict[k].ndim == 0: t = state_dict[k].reshape(1).cpu().numpy()\n",
    "        else: t = state_dict[k].cpu().numpy()\n",
    "        model_state_dict[k].assign(Tensor(t)).realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model_inputs = {}\n",
    "with safe_open(\"world_model_inputs.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        world_model_inputs[k] = f.get_tensor(k)\n",
    "\n",
    "world_model_inputs = {k: Tensor(v.cpu().numpy()) for k, v in world_model_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WorldModel' object has no attribute 'zero_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mworld_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Tensor\u001b[38;5;241m.\u001b[39mtrain():\n\u001b[1;32m      3\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m world_model\u001b[38;5;241m.\u001b[39mloss(world_model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m], world_model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m], world_model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m], world_model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtermination\u001b[39m\u001b[38;5;124m\"\u001b[39m],logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WorldModel' object has no attribute 'zero_grad'"
     ]
    }
   ],
   "source": [
    "with Tensor.train():\n",
    "    total_loss = world_model.loss(world_model_inputs[\"obs\"], world_model_inputs[\"action\"], world_model_inputs[\"reward\"], world_model_inputs[\"termination\"],logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model_outputs = {}\n",
    "with safe_open(\"world_model_outputs.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        world_model_outputs[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6644.0435, device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_model_outputs[\"total_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(6646.534, dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
