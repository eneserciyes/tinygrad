{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from omegaconf import OmegaConf\n",
    "import gymnasium\n",
    "from world_models import WorldModel\n",
    "\n",
    "import colorama\n",
    "import numpy as np\n",
    "from tinygrad import Tensor, nn\n",
    "from tinygrad.nn.state import get_parameters, get_state_dict\n",
    "\n",
    "import env_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_env(env_name, image_size, seed):\n",
    "  env = gymnasium.make(env_name, full_action_space=False, render_mode=\"rgb_array\", frameskip=1)\n",
    "  env = env_wrapper.SeedEnvWrapper(env, seed=seed)\n",
    "  env = env_wrapper.MaxLast2FrameSkipWrapper(env, skip=4)\n",
    "  env = gymnasium.wrappers.ResizeObservation(env, shape=image_size)\n",
    "  env = env_wrapper.LifeLossInfo(env)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_world_model(conf, action_dim):\n",
    "    return WorldModel(\n",
    "        in_channels=conf.models.world_model.in_channels,\n",
    "        action_dim=action_dim,\n",
    "        transformer_max_length=conf.models.world_model.transformer_max_length,\n",
    "        transformer_hidden_dim=conf.models.world_model.transformer_hidden_dim,\n",
    "        transformer_num_layers=conf.models.world_model.transformer_num_layers,\n",
    "        transformer_num_heads=conf.models.world_model.transformer_num_heads\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-n\", type=str, required=True)\n",
    "parser.add_argument(\"-seed\", type=int, required=True)\n",
    "parser.add_argument(\"-config_path\", type=str, required=True)\n",
    "parser.add_argument(\"-env_name\", type=str, required=True)\n",
    "parser.add_argument(\"-trajectory_path\", type=str, required=True)\n",
    "args = parser.parse_args([\n",
    "    \"-n\", \"MsPacman\",\n",
    "    \"-seed\", \"1\",\n",
    "    \"-config_path\", \"STORM.yaml\",\n",
    "    \"-env_name\", \"ALE/MsPacman-v5\",\n",
    "    \"-trajectory_path\", \"D_TRAJ/MsPacman.pkl\"\n",
    "])\n",
    "conf = OmegaConf.load(args.config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mNamespace(n='MsPacman', seed=1, config_path='STORM.yaml', env_name='ALE/MsPacman-v5', trajectory_path='D_TRAJ/MsPacman.pkl')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colorama.Fore.RED + str(args) + colorama.Style.RESET_ALL)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "Tensor.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "dummy_env = build_single_env(args.env_name, conf.basic_settings.image_size, seed=0)\n",
    "action_dim = dummy_env.action_space.n.item()\n",
    "del dummy_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model = build_world_model(conf, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# updatable_count = 0\n",
    "# for k, v in get_state_dict(world_model).items():\n",
    "#     print(i, \"- \", k, \": \", v.shape, \" -- \", v.requires_grad)\n",
    "#     if v.requires_grad is None: updatable_count += 1\n",
    "#     i+=1\n",
    "# print(\"Total updatable parameters: \", updatable_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World model safetensors from torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {}\n",
    "with safe_open(\"world_model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        state_dict[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dist_head.post_head.bias', 'dist_head.post_head.weight', 'dist_head.prior_head.bias', 'dist_head.prior_head.weight', 'encoder.backbone.0.weight', 'encoder.backbone.1.bias', 'encoder.backbone.1.num_batches_tracked', 'encoder.backbone.1.running_mean', 'encoder.backbone.1.running_var', 'encoder.backbone.1.weight', 'encoder.backbone.10.bias', 'encoder.backbone.10.num_batches_tracked', 'encoder.backbone.10.running_mean', 'encoder.backbone.10.running_var', 'encoder.backbone.10.weight', 'encoder.backbone.3.weight', 'encoder.backbone.4.bias', 'encoder.backbone.4.num_batches_tracked', 'encoder.backbone.4.running_mean', 'encoder.backbone.4.running_var', 'encoder.backbone.4.weight', 'encoder.backbone.6.weight', 'encoder.backbone.7.bias', 'encoder.backbone.7.num_batches_tracked', 'encoder.backbone.7.running_mean', 'encoder.backbone.7.running_var', 'encoder.backbone.7.weight', 'encoder.backbone.9.weight', 'image_decoder.backbone.0.weight', 'image_decoder.backbone.10.weight', 'image_decoder.backbone.11.bias', 'image_decoder.backbone.11.num_batches_tracked', 'image_decoder.backbone.11.running_mean', 'image_decoder.backbone.11.running_var', 'image_decoder.backbone.11.weight', 'image_decoder.backbone.13.bias', 'image_decoder.backbone.13.weight', 'image_decoder.backbone.2.bias', 'image_decoder.backbone.2.num_batches_tracked', 'image_decoder.backbone.2.running_mean', 'image_decoder.backbone.2.running_var', 'image_decoder.backbone.2.weight', 'image_decoder.backbone.4.weight', 'image_decoder.backbone.5.bias', 'image_decoder.backbone.5.num_batches_tracked', 'image_decoder.backbone.5.running_mean', 'image_decoder.backbone.5.running_var', 'image_decoder.backbone.5.weight', 'image_decoder.backbone.7.weight', 'image_decoder.backbone.8.bias', 'image_decoder.backbone.8.num_batches_tracked', 'image_decoder.backbone.8.running_mean', 'image_decoder.backbone.8.running_var', 'image_decoder.backbone.8.weight', 'reward_decoder.backbone.0.weight', 'reward_decoder.backbone.1.bias', 'reward_decoder.backbone.1.weight', 'reward_decoder.backbone.3.weight', 'reward_decoder.backbone.4.bias', 'reward_decoder.backbone.4.weight', 'reward_decoder.head.bias', 'reward_decoder.head.weight', 'storm_transformer.layer_norm.bias', 'storm_transformer.layer_norm.weight', 'storm_transformer.layer_stack.0.pos_ffn.layer_norm.bias', 'storm_transformer.layer_stack.0.pos_ffn.layer_norm.weight', 'storm_transformer.layer_stack.0.pos_ffn.w_1.bias', 'storm_transformer.layer_stack.0.pos_ffn.w_1.weight', 'storm_transformer.layer_stack.0.pos_ffn.w_2.bias', 'storm_transformer.layer_stack.0.pos_ffn.w_2.weight', 'storm_transformer.layer_stack.0.slf_attn.fc.weight', 'storm_transformer.layer_stack.0.slf_attn.layer_norm.bias', 'storm_transformer.layer_stack.0.slf_attn.layer_norm.weight', 'storm_transformer.layer_stack.0.slf_attn.w_ks.weight', 'storm_transformer.layer_stack.0.slf_attn.w_qs.weight', 'storm_transformer.layer_stack.0.slf_attn.w_vs.weight', 'storm_transformer.layer_stack.1.pos_ffn.layer_norm.bias', 'storm_transformer.layer_stack.1.pos_ffn.layer_norm.weight', 'storm_transformer.layer_stack.1.pos_ffn.w_1.bias', 'storm_transformer.layer_stack.1.pos_ffn.w_1.weight', 'storm_transformer.layer_stack.1.pos_ffn.w_2.bias', 'storm_transformer.layer_stack.1.pos_ffn.w_2.weight', 'storm_transformer.layer_stack.1.slf_attn.fc.weight', 'storm_transformer.layer_stack.1.slf_attn.layer_norm.bias', 'storm_transformer.layer_stack.1.slf_attn.layer_norm.weight', 'storm_transformer.layer_stack.1.slf_attn.w_ks.weight', 'storm_transformer.layer_stack.1.slf_attn.w_qs.weight', 'storm_transformer.layer_stack.1.slf_attn.w_vs.weight', 'storm_transformer.position_encoding.pos_emb.weight', 'storm_transformer.stem.0.weight', 'storm_transformer.stem.1.bias', 'storm_transformer.stem.1.weight', 'storm_transformer.stem.3.weight', 'storm_transformer.stem.4.bias', 'storm_transformer.stem.4.weight', 'termination_decoder.backbone.0.weight', 'termination_decoder.backbone.1.bias', 'termination_decoder.backbone.1.weight', 'termination_decoder.backbone.3.weight', 'termination_decoder.backbone.4.bias', 'termination_decoder.backbone.4.weight', 'termination_decoder.head.0.bias', 'termination_decoder.head.0.weight'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB CUDA (1024,) contig:True (<LoadOps.COPY: 3>, None)> on CUDA with grad None>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(state_dict['dist_head.post_head.bias'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.backbone.0.weight  --  yes\n",
      "encoder.backbone.1.weight  --  yes\n",
      "encoder.backbone.1.bias  --  yes\n",
      "encoder.backbone.1.running_mean  --  yes\n",
      "encoder.backbone.1.running_var  --  yes\n",
      "encoder.backbone.1.num_batches_tracked  --  yes\n",
      "encoder.backbone.3.weight  --  yes\n",
      "encoder.backbone.4.weight  --  yes\n",
      "encoder.backbone.4.bias  --  yes\n",
      "encoder.backbone.4.running_mean  --  yes\n",
      "encoder.backbone.4.running_var  --  yes\n",
      "encoder.backbone.4.num_batches_tracked  --  yes\n",
      "encoder.backbone.6.weight  --  yes\n",
      "encoder.backbone.7.weight  --  yes\n",
      "encoder.backbone.7.bias  --  yes\n",
      "encoder.backbone.7.running_mean  --  yes\n",
      "encoder.backbone.7.running_var  --  yes\n",
      "encoder.backbone.7.num_batches_tracked  --  yes\n",
      "encoder.backbone.9.weight  --  yes\n",
      "encoder.backbone.10.weight  --  yes\n",
      "encoder.backbone.10.bias  --  yes\n",
      "encoder.backbone.10.running_mean  --  yes\n",
      "encoder.backbone.10.running_var  --  yes\n",
      "encoder.backbone.10.num_batches_tracked  --  yes\n",
      "storm_transformer.stem.0.weight  --  yes\n",
      "storm_transformer.stem.1.weight  --  yes\n",
      "storm_transformer.stem.1.bias  --  yes\n",
      "storm_transformer.stem.3.weight  --  yes\n",
      "storm_transformer.stem.4.weight  --  yes\n",
      "storm_transformer.stem.4.bias  --  yes\n",
      "storm_transformer.position_encoding.pos_emb.weight  --  yes\n",
      "storm_transformer.layer_stack.0.slf_attn.w_qs.weight  --  yes\n",
      "storm_transformer.layer_stack.0.slf_attn.w_ks.weight  --  yes\n",
      "storm_transformer.layer_stack.0.slf_attn.w_vs.weight  --  yes\n",
      "storm_transformer.layer_stack.0.slf_attn.fc.weight  --  yes\n",
      "storm_transformer.layer_stack.0.slf_attn.layer_norm.weight  --  yes\n",
      "storm_transformer.layer_stack.0.slf_attn.layer_norm.bias  --  yes\n",
      "storm_transformer.layer_stack.0.pos_ffn.w_1.weight  --  yes\n",
      "storm_transformer.layer_stack.0.pos_ffn.w_1.bias  --  yes\n",
      "storm_transformer.layer_stack.0.pos_ffn.w_2.weight  --  yes\n",
      "storm_transformer.layer_stack.0.pos_ffn.w_2.bias  --  yes\n",
      "storm_transformer.layer_stack.0.pos_ffn.layer_norm.weight  --  yes\n",
      "storm_transformer.layer_stack.0.pos_ffn.layer_norm.bias  --  yes\n",
      "storm_transformer.layer_stack.1.slf_attn.w_qs.weight  --  yes\n",
      "storm_transformer.layer_stack.1.slf_attn.w_ks.weight  --  yes\n",
      "storm_transformer.layer_stack.1.slf_attn.w_vs.weight  --  yes\n",
      "storm_transformer.layer_stack.1.slf_attn.fc.weight  --  yes\n",
      "storm_transformer.layer_stack.1.slf_attn.layer_norm.weight  --  yes\n",
      "storm_transformer.layer_stack.1.slf_attn.layer_norm.bias  --  yes\n",
      "storm_transformer.layer_stack.1.pos_ffn.w_1.weight  --  yes\n",
      "storm_transformer.layer_stack.1.pos_ffn.w_1.bias  --  yes\n",
      "storm_transformer.layer_stack.1.pos_ffn.w_2.weight  --  yes\n",
      "storm_transformer.layer_stack.1.pos_ffn.w_2.bias  --  yes\n",
      "storm_transformer.layer_stack.1.pos_ffn.layer_norm.weight  --  yes\n",
      "storm_transformer.layer_stack.1.pos_ffn.layer_norm.bias  --  yes\n",
      "storm_transformer.layer_norm.weight  --  yes\n",
      "storm_transformer.layer_norm.bias  --  yes\n",
      "dist_head.post_head.weight  --  yes\n",
      "dist_head.post_head.bias  --  yes\n",
      "dist_head.prior_head.weight  --  yes\n",
      "dist_head.prior_head.bias  --  yes\n",
      "image_decoder.backbone.0.weight  --  yes\n",
      "image_decoder.backbone.2.weight  --  yes\n",
      "image_decoder.backbone.2.bias  --  yes\n",
      "image_decoder.backbone.2.running_mean  --  yes\n",
      "image_decoder.backbone.2.running_var  --  yes\n",
      "image_decoder.backbone.2.num_batches_tracked  --  yes\n",
      "image_decoder.backbone.4.weight  --  yes\n",
      "image_decoder.backbone.5.weight  --  yes\n",
      "image_decoder.backbone.5.bias  --  yes\n",
      "image_decoder.backbone.5.running_mean  --  yes\n",
      "image_decoder.backbone.5.running_var  --  yes\n",
      "image_decoder.backbone.5.num_batches_tracked  --  yes\n",
      "image_decoder.backbone.7.weight  --  yes\n",
      "image_decoder.backbone.8.weight  --  yes\n",
      "image_decoder.backbone.8.bias  --  yes\n",
      "image_decoder.backbone.8.running_mean  --  yes\n",
      "image_decoder.backbone.8.running_var  --  yes\n",
      "image_decoder.backbone.8.num_batches_tracked  --  yes\n",
      "image_decoder.backbone.10.weight  --  yes\n",
      "image_decoder.backbone.11.weight  --  yes\n",
      "image_decoder.backbone.11.bias  --  yes\n",
      "image_decoder.backbone.11.running_mean  --  yes\n",
      "image_decoder.backbone.11.running_var  --  yes\n",
      "image_decoder.backbone.11.num_batches_tracked  --  yes\n",
      "image_decoder.backbone.13.weight  --  yes\n",
      "image_decoder.backbone.13.bias  --  yes\n",
      "reward_decoder.backbone.0.weight  --  yes\n",
      "reward_decoder.backbone.1.weight  --  yes\n",
      "reward_decoder.backbone.1.bias  --  yes\n",
      "reward_decoder.backbone.3.weight  --  yes\n",
      "reward_decoder.backbone.4.weight  --  yes\n",
      "reward_decoder.backbone.4.bias  --  yes\n",
      "reward_decoder.head.weight  --  yes\n",
      "reward_decoder.head.bias  --  yes\n",
      "termination_decoder.backbone.0.weight  --  yes\n",
      "termination_decoder.backbone.1.weight  --  yes\n",
      "termination_decoder.backbone.1.bias  --  yes\n",
      "termination_decoder.backbone.3.weight  --  yes\n",
      "termination_decoder.backbone.4.weight  --  yes\n",
      "termination_decoder.backbone.4.bias  --  yes\n",
      "termination_decoder.head.0.weight  --  yes\n",
      "termination_decoder.head.0.bias  --  yes\n",
      "symlog_twohot_loss_func.bins  --  no\n"
     ]
    }
   ],
   "source": [
    "for k, v in get_state_dict(world_model).items():\n",
    "    if k in state_dict.keys():\n",
    "        print(k, \" -- \", \"yes\")\n",
    "    else:\n",
    "        print(k, \" -- \", \"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16510730"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x.numel() for x in get_parameters(world_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_state_dict = get_state_dict(world_model)\n",
    "for k, v in tqdm(model_state_dict.items()):\n",
    "    if k in state_dict.keys():\n",
    "        if state_dict[k].ndim == 0: t = state_dict[k].reshape(1).cpu().numpy()\n",
    "        else: t = state_dict[k].cpu().numpy()\n",
    "        model_state_dict[k].assign(Tensor(t)).realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model_inputs = {}\n",
    "with safe_open(\"world_model_inputs.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        world_model_inputs[k] = f.get_tensor(k)\n",
    "\n",
    "world_model_inputs = {k: Tensor(v.cpu().numpy()) for k, v in world_model_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tensor.train():\n",
    "    total_loss = world_model.loss(world_model_inputs[\"obs\"], world_model_inputs[\"action\"], world_model_inputs[\"reward\"], world_model_inputs[\"termination\"],logger=None)\n",
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model_outputs = {}\n",
    "with safe_open(\"world_model_outputs.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        world_model_outputs[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_out = {}\n",
    "with safe_open(\"grad_out.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        grad_out[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_out[\"encoder.backbone.0.weight.grad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_out[\"termination_decoder.backbone.0.weight.grad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model.encoder.backbone[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model_outputs[\"total_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.nn.state import safe_load\n",
    "wm_out_tiny = safe_load(\"world_model_output_embed_post_logits.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm_out_tiny[\"embedding\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model_outputs[\"embedding\"].cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
